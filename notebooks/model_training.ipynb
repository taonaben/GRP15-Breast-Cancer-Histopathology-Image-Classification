{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561ea8d-5740-423b-a403-b435f665df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pyarrow\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9b1325-521b-4df3-9a7b-9b72e0faacf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\taona\\anaconda3\\lib\\site-packages (6.28.0)\n",
      "Collecting ipykernel\n",
      "  Using cached ipykernel-7.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: jupyter in c:\\users\\taona\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Collecting jupyter\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: notebook in c:\\users\\taona\\anaconda3\\lib\\site-packages (7.0.8)\n",
      "Collecting notebook\n",
      "  Using cached notebook-7.4.7-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (8.25.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (23.2)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter) (7.10.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter) (7.8.1)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter) (4.0.11)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from notebook) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from notebook) (2.27.3)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Using cached jupyterlab-4.4.10-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from notebook) (0.2.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (305.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.4)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.4.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.9.2)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (2.0.10)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab->jupyter) (2.0.4)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab->jupyter) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab->jupyter) (2.2.0)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab->jupyter) (69.5.1)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.6 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (3.6.6)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (1.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\taona\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->notebook) (2024.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\taona\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\taona\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\taona\\anaconda3\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.10.6)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\taona\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\taona\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\taona\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\taona\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\taona\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.1)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook)\n",
      "  Using cached webcolors-25.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\taona\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\taona\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.2.3)\n",
      "Using cached ipykernel-7.1.0-py3-none-any.whl (117 kB)\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading notebook-7.4.7-py3-none-any.whl (14.3 MB)\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.3 MB 93.5 kB/s eta 0:02:34\n",
      "   ---------------------------------------- 0.0/14.3 MB 93.5 kB/s eta 0:02:34\n",
      "   ---------------------------------------- 0.0/14.3 MB 89.3 kB/s eta 0:02:41\n",
      "   ---------------------------------------- 0.0/14.3 MB 89.3 kB/s eta 0:02:41\n",
      "   ---------------------------------------- 0.1/14.3 MB 125.8 kB/s eta 0:01:54\n",
      "   ---------------------------------------- 0.1/14.3 MB 131.0 kB/s eta 0:01:49\n",
      "   ---------------------------------------- 0.1/14.3 MB 192.5 kB/s eta 0:01:14\n",
      "   ---------------------------------------- 0.1/14.3 MB 200.1 kB/s eta 0:01:12\n",
      "   ---------------------------------------- 0.2/14.3 MB 262.1 kB/s eta 0:00:55\n",
      "    --------------------------------------- 0.2/14.3 MB 289.5 kB/s eta 0:00:49\n",
      "    --------------------------------------- 0.2/14.3 MB 305.9 kB/s eta 0:00:47\n",
      "    --------------------------------------- 0.2/14.3 MB 294.1 kB/s eta 0:00:48\n",
      "    --------------------------------------- 0.3/14.3 MB 403.1 kB/s eta 0:00:35\n",
      "   - -------------------------------------- 0.4/14.3 MB 464.4 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.5/14.3 MB 519.3 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.5/14.3 MB 519.3 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 0.6/14.3 MB 553.0 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.6/14.3 MB 606.4 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.8/14.3 MB 767.5 kB/s eta 0:00:18\n",
      "   -- ------------------------------------- 1.0/14.3 MB 879.5 kB/s eta 0:00:16\n",
      "   -- ------------------------------------- 1.0/14.3 MB 879.5 kB/s eta 0:00:16\n",
      "   --- ------------------------------------ 1.1/14.3 MB 887.2 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 1.2/14.3 MB 958.5 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 1.4/14.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.8/14.3 MB 1.3 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.1/14.3 MB 1.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.2/14.3 MB 1.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.4/14.3 MB 1.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 2.7/14.3 MB 1.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.1/14.3 MB 1.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.6/14.3 MB 2.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 4.0/14.3 MB 2.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.2/14.3 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 4.5/14.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.0/14.3 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.4/14.3 MB 2.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.0/14.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.4/14.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.9/14.3 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.3/14.3 MB 3.5 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.7/14.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.9/14.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.4/14.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 8.8/14.3 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 9.0/14.3 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.7/14.3 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 10.0/14.3 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.4/14.3 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.8/14.3 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.1/14.3 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.7/14.3 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.1/14.3 MB 8.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.6/14.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.0/14.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.5/14.3 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.9/14.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.3/14.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.3/14.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.3/14.3 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading jupyterlab-4.4.10-py3-none-any.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.6/12.3 MB 18.2 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.2/12.3 MB 14.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.7/12.3 MB 13.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.3/12.3 MB 13.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.6/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.2/12.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.6/12.3 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.2/12.3 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.9/12.3 MB 12.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.3/12.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.0/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.9/12.3 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.3 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.3 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.3 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.3 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.7/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.4/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.0/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webcolors, uri-template, fqdn, isoduration, ipykernel, jupyterlab, notebook, jupyter\n",
      "  Attempting uninstall: ipykernel\n",
      "    Found existing installation: ipykernel 6.28.0\n",
      "    Uninstalling ipykernel-6.28.0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pyarrow-14.0.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pyarrow-14.0.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\streamlit-1.32.0.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pyarrow-14.0.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\streamlit-1.32.0.dist-info due to invalid metadata entry 'name'\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified\n",
      "\n",
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pyarrow-14.0.2.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\jupyterlab-4.0.11.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ipykernel jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d2f4f1-e4eb-46f2-883c-381c8dc446fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy\n",
    "\n",
    "!pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708788de-c895-4fde-9543-ecde503d39c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    \"\"\"Initialize the application.\"\"\"\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    \"\"\"Schedule the next advance of the eventloop\"\"\"\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    subshell_id\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    # Remove for ipykernel 5.0\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    else:\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\taona\\AppData\\Local\\Temp\\ipykernel_5988\\1320214373.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"C:\\Users\\taona\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     ArrowDtype,\n\u001b[0;32m     52\u001b[0m     Int8Dtype,\n\u001b[0;32m     53\u001b[0m     Int16Dtype,\n\u001b[0;32m     54\u001b[0m     Int32Dtype,\n\u001b[0;32m     55\u001b[0m     Int64Dtype,\n\u001b[0;32m     56\u001b[0m     UInt8Dtype,\n\u001b[0;32m     57\u001b[0m     UInt16Dtype,\n\u001b[0;32m     58\u001b[0m     UInt32Dtype,\n\u001b[0;32m     59\u001b[0m     UInt64Dtype,\n\u001b[0;32m     60\u001b[0m     Float32Dtype,\n\u001b[0;32m     61\u001b[0m     Float64Dtype,\n\u001b[0;32m     62\u001b[0m     CategoricalDtype,\n\u001b[0;32m     63\u001b[0m     PeriodDtype,\n\u001b[0;32m     64\u001b[0m     IntervalDtype,\n\u001b[0;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     66\u001b[0m     StringDtype,\n\u001b[0;32m     67\u001b[0m     BooleanDtype,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     NA,\n\u001b[0;32m     70\u001b[0m     isna,\n\u001b[0;32m     71\u001b[0m     isnull,\n\u001b[0;32m     72\u001b[0m     notna,\n\u001b[0;32m     73\u001b[0m     notnull,\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     Index,\n\u001b[0;32m     76\u001b[0m     CategoricalIndex,\n\u001b[0;32m     77\u001b[0m     RangeIndex,\n\u001b[0;32m     78\u001b[0m     MultiIndex,\n\u001b[0;32m     79\u001b[0m     IntervalIndex,\n\u001b[0;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     81\u001b[0m     DatetimeIndex,\n\u001b[0;32m     82\u001b[0m     PeriodIndex,\n\u001b[0;32m     83\u001b[0m     IndexSlice,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     NaT,\n\u001b[0;32m     86\u001b[0m     Period,\n\u001b[0;32m     87\u001b[0m     period_range,\n\u001b[0;32m     88\u001b[0m     Timedelta,\n\u001b[0;32m     89\u001b[0m     timedelta_range,\n\u001b[0;32m     90\u001b[0m     Timestamp,\n\u001b[0;32m     91\u001b[0m     date_range,\n\u001b[0;32m     92\u001b[0m     bdate_range,\n\u001b[0;32m     93\u001b[0m     Interval,\n\u001b[0;32m     94\u001b[0m     interval_range,\n\u001b[0;32m     95\u001b[0m     DateOffset,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     to_numeric,\n\u001b[0;32m     98\u001b[0m     to_datetime,\n\u001b[0;32m     99\u001b[0m     to_timedelta,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     Flags,\n\u001b[0;32m    102\u001b[0m     Grouper,\n\u001b[0;32m    103\u001b[0m     factorize,\n\u001b[0;32m    104\u001b[0m     unique,\n\u001b[0;32m    105\u001b[0m     value_counts,\n\u001b[0;32m    106\u001b[0m     NamedAgg,\n\u001b[0;32m    107\u001b[0m     array,\n\u001b[0;32m    108\u001b[0m     Categorical,\n\u001b[0;32m    109\u001b[0m     set_eng_float_format,\n\u001b[0;32m    110\u001b[0m     Series,\n\u001b[0;32m    111\u001b[0m     DataFrame,\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, BatchNormalization, Dropout\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232eaa64-f7de-4c67-84ee-ca546cc72dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36a706-76c7-4676-878c-8ec414e6084b",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "## Set up paths and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782dd01-b987-458c-b23d-03d2ff036114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data paths\n",
    "base_dir = '../data/raw/archive/'\n",
    "csv_path = 'Folds.csv'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(base_dir + csv_path)\n",
    "\n",
    "# Process file paths - rename the filename column to path\n",
    "df = df.rename(columns={'filename': 'path'})\n",
    "\n",
    "# Check what the path column contains\n",
    "print(\"Sample paths from CSV:\")\n",
    "print(df['path'].head(3))\n",
    "print(\"\\nFirst path structure:\", df['path'].iloc[0] if len(df) > 0 else \"No data\")\n",
    "\n",
    "# Clean up the path: remove any leading/trailing BreaKHis_v1 to avoid duplication\n",
    "# The CSV paths likely contain: \"BreaKHis_v1/histology_slides/breast/...\" \n",
    "# or just \"histology_slides/breast/...\" or \"benign/...\" or \"malignant/...\"\n",
    "first_path = df['path'].iloc[0] if len(df) > 0 else \"\"\n",
    "\n",
    "# Remove BreaKHis_v1 from the beginning of paths if present\n",
    "df['path_clean'] = df['path'].str.replace(r'^BreaKHis_v1/', '', regex=True)\n",
    "df['path_clean'] = df['path_clean'].str.replace(r'^/BreaKHis_v1/', '/', regex=True)\n",
    "\n",
    "# Determine img_dir based on what remains\n",
    "path_clean_first = df['path_clean'].iloc[0] if len(df) > 0 else \"\"\n",
    "\n",
    "if path_clean_first.startswith('histology_slides'):\n",
    "    # Path starts with histology_slides, need to add BreaKHis_v1\n",
    "    img_dir = os.path.join(base_dir, 'BreaKHis_v1')\n",
    "elif path_clean_first.startswith(('benign', 'malignant')):\n",
    "    # Path starts with class name, need full path\n",
    "    img_dir = os.path.join(base_dir, 'BreaKHis_v1', 'histology_slides', 'breast')\n",
    "else:\n",
    "    # Default: assume path needs BreaKHis_v1\n",
    "    img_dir = os.path.join(base_dir, 'BreaKHis_v1')\n",
    "\n",
    "# Normalize the path separator for Windows compatibility\n",
    "img_dir = img_dir.replace('\\\\', '/')\n",
    "\n",
    "# Construct full file paths\n",
    "df['filepath'] = img_dir + '/' + df['path_clean']\n",
    "\n",
    "# Clean up any double slashes\n",
    "df['filepath'] = df['filepath'].str.replace('//', '/', regex=False)\n",
    "df['filepath'] = df['filepath'].str.replace('//', '/', regex=False)  # Do twice to handle triple slashes\n",
    "\n",
    "# Verify the path construction\n",
    "print(f\"\\nImage directory: {img_dir}\")\n",
    "print(f\"Sample cleaned path: {df['path_clean'].iloc[0] if len(df) > 0 else 'No data'}\")\n",
    "print(f\"Sample filepath: {df['filepath'].iloc[0] if len(df) > 0 else 'No data'}\")\n",
    "\n",
    "# Check if files exist (sample check on multiple files)\n",
    "if len(df) > 0:\n",
    "    sample_paths = df['filepath'].head(5).tolist()\n",
    "    existing_count = sum(1 for p in sample_paths if os.path.exists(p))\n",
    "    print(f\"\\nFiles found: {existing_count}/{len(sample_paths)} sample files exist\")\n",
    "    \n",
    "    if existing_count == 0:\n",
    "        print(\"\\n⚠ WARNING: No sample files found!\")\n",
    "        print(\"Trying alternative path construction...\")\n",
    "        \n",
    "        # Try alternative: if CSV has full path from archive, use base_dir directly\n",
    "        alt_img_dir = base_dir.rstrip('/')\n",
    "        df['filepath_alt'] = alt_img_dir + '/' + df['path_clean']\n",
    "        df['filepath_alt'] = df['filepath_alt'].str.replace('//', '/', regex=False)\n",
    "        \n",
    "        alt_existing = sum(1 for p in df['filepath_alt'].head(5) if os.path.exists(p))\n",
    "        if alt_existing > 0:\n",
    "            print(f\"✓ Alternative path works! Found {alt_existing}/5 files\")\n",
    "            df['filepath'] = df['filepath_alt']\n",
    "            img_dir = alt_img_dir\n",
    "        else:\n",
    "            print(\"✗ Alternative path also failed.\")\n",
    "            print(\"\\nPlease check:\")\n",
    "            print(f\"  1. CSV path structure: {df['path'].iloc[0]}\")\n",
    "            print(f\"  2. Expected image location: {img_dir}\")\n",
    "            print(f\"  3. Verify files exist in: {base_dir}\")\n",
    "\n",
    "# Extract labels from the cleaned path\n",
    "df['label'] = df['path_clean'].apply(lambda x: 'benign' if 'benign' in x.lower() else 'malignant')\n",
    "df['label_int'] = df['label'].apply(lambda x: 1 if x == 'benign' else 0)\n",
    "df['filename'] = df['path_clean'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "# Use the cleaned path for the path column going forward\n",
    "df['path'] = df['path_clean']\n",
    "df = df.drop('path_clean', axis=1, errors='ignore')\n",
    "if 'filepath_alt' in df.columns:\n",
    "    df = df.drop('filepath_alt', axis=1, errors='ignore')\n",
    "\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"Benign: {df[df.label == 'benign'].shape[0]}\")\n",
    "print(f\"Malignant: {df[df.label == 'malignant'].shape[0]}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d708af-1f28-43d2-b05c-a38c0bc703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "# First, create test set with balanced sampling\n",
    "test_df = df.groupby('label').sample(n=300, random_state=42).reset_index(drop=True)\n",
    "remaining_df = df.drop(test_df.index).reset_index(drop=True)\n",
    "\n",
    "# Split remaining data into train and validation (80/20)\n",
    "train_df, valid_df = train_test_split(\n",
    "    remaining_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=remaining_df['label']\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "# Handle class imbalance in training set using oversampling\n",
    "max_count = train_df.label.value_counts().max()\n",
    "train_df_balanced = train_df.groupby('label').sample(n=max_count, replace=True, random_state=42).reset_index(drop=True)\n",
    "train_df = train_df_balanced\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "print(train_df.label.value_counts())\n",
    "print(\"\\nValidation set distribution:\")\n",
    "print(valid_df.label.value_counts())\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(test_df.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bf7dd-5a53-4913-8917-c585bbcc74c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_df[\"filepath\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c823a-721c-4ff5-8fc0-af4c1c7ecbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing functions\n",
    "def parse_image(img_path, label):\n",
    "    \"\"\"Function to parse and load images with their labels\"\"\"\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    return img, label\n",
    "\n",
    "def resize_rescale(image, label):\n",
    "    \"\"\"Function to resize and rescale images (for validation/test)\"\"\"\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "def augmentor(image, label):\n",
    "    \"\"\"Function to apply data augmentation (for training only)\"\"\"\n",
    "    # Resize first\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    \n",
    "    # Normalize to [0, 1] range first (images are in [0, 255])\n",
    "    image = image / 255.0\n",
    "    \n",
    "    # Data augmentation techniques to prevent overfitting\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_hue(image, max_delta=0.1)\n",
    "    \n",
    "    # Add random rotation (90 degree increments)\n",
    "    image = tf.image.rot90(image, k=tf.random.uniform([], 0, 4, dtype=tf.int32))\n",
    "    \n",
    "    # Ensure values stay in [0, 1] range after augmentation\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Create TF datasets\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "train_loader = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df.filepath.values, train_df.label_int.values)\n",
    ")\n",
    "valid_loader = tf.data.Dataset.from_tensor_slices(\n",
    "    (valid_df.filepath.values, valid_df.label_int.values)\n",
    ")\n",
    "test_loader = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.filepath.values, test_df.label_int.values)\n",
    ")\n",
    "\n",
    "# Create efficient data pipelines\n",
    "train_ds = (\n",
    "    train_loader\n",
    "    .shuffle(buffer_size=len(train_df), seed=42)\n",
    "    .map(parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(augmentor, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_ds = (\n",
    "    valid_loader\n",
    "    .map(parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(resize_rescale, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    test_loader\n",
    "    .map(parse_image, num_parallel_calls=AUTOTUNE)\n",
    "    .map(resize_rescale, num_parallel_calls=AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_ds)}\")\n",
    "print(f\"Validation batches: {len(valid_ds)}\")\n",
    "print(f\"Test batches: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56161cc-bc67-4169-b910-a3b1d57227f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_image(dataset, num_images=4):\n",
    "    \"\"\"Display sample images from the dataset\"\"\"\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(num_images):\n",
    "            axes[i].imshow(images[i].numpy())\n",
    "            axes[i].set_title(f'Label: {labels[i].numpy()}')\n",
    "            axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Before running this function, make sure the dataset is properly loaded\n",
    "# Check if train_ds exists and contains valid data\n",
    "try:\n",
    "    # Verify dataset exists and has data\n",
    "    sample = next(iter(train_ds))\n",
    "    # If successful, view images\n",
    "    view_image(train_ds, num_images=4)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please check that:\")\n",
    "    print(\"1. The dataset path is correct\")\n",
    "    print(\"2. The dataset has been properly loaded\")\n",
    "    print(\"3. The images exist at the specified location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0250b0-f7e5-4d52-bfd3-d1db63eac82c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae6cd44e-f03c-489a-b931-a0a6c199d1e1",
   "metadata": {},
   "source": [
    "### Building the Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758302a-eb1d-4712-abdf-6e0642273bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Create the base model as a Keras Layer\n",
    "base_model = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\",\n",
    "    trainable=False, \n",
    "    name='efficientnetv2-b0'\n",
    ")\n",
    "\n",
    "# Build model with improved overfitting prevention\n",
    "model = Sequential([\n",
    "    Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='input_layer'),\n",
    "    \n",
    "    # Wrap the base_model in a functional API to make it compatible with Sequential\n",
    "    tf.keras.layers.Lambda(lambda x: base_model(x), name='base_model_wrapper'),\n",
    "    \n",
    "    # Add dropout right after base model to prevent overfitting\n",
    "    Dropout(0.2, name='dropout_base'),\n",
    "    BatchNormalization(name='bn_base'),\n",
    "    \n",
    "    # First dense layer with regularization\n",
    "    Dense(512, activation='relu', \n",
    "          kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "          bias_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "          name='dense_1'),\n",
    "    BatchNormalization(name='bn_1'),\n",
    "    Dropout(0.5, name='dropout_1'),\n",
    "    \n",
    "    # Second dense layer with regularization\n",
    "    Dense(256, activation='relu',\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "          bias_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "          name='dense_2'),\n",
    "    BatchNormalization(name='bn_2'),\n",
    "    Dropout(0.4, name='dropout_2'),\n",
    "    \n",
    "    # Third dense layer\n",
    "    Dense(128, activation='relu',\n",
    "          kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "          name='dense_3'),\n",
    "    BatchNormalization(name='bn_3'),\n",
    "    Dropout(0.3, name='dropout_3'),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid', name='output_layer')\n",
    "], name='breast_cancer_classifier')\n",
    "\n",
    "# Use a lower initial learning rate with ReduceLROnPlateau callback\n",
    "# This will be set in callbacks, using a moderate initial LR here\n",
    "initial_learning_rate = 0.0001\n",
    "\n",
    "# Compile with appropriate metrics\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=initial_learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    ),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1),  # Label smoothing for regularization\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3d1d0-1fa1-40d5-8c7b-7182650e813a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b440b5f-f5aa-4fd8-b049-8b26fbe63b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging directory\n",
    "import datetime\n",
    "logdir = 'logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(logdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824c228-052b-471f-bce4-0ba178efb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks for better training and overfitting prevention\n",
    "import os\n",
    "\n",
    "# TensorBoard callback for visualization\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0\n",
    ")\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,  # Wait 7 epochs before stopping\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Reduce learning rate on plateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,  # Reduce LR by half\n",
    "    patience=3,  # Wait 3 epochs\n",
    "    min_lr=1e-7,  # Minimum learning rate\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Model checkpoint to save best model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'models/best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# CSV logger to track training progress\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(\n",
    "    'models/training_log.csv',\n",
    "    append=False\n",
    ")\n",
    "\n",
    "# Compile callbacks list\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    early_stopping,\n",
    "    reduce_lr,\n",
    "    model_checkpoint,\n",
    "    csv_logger\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(f\"  - TensorBoard: {logdir}\")\n",
    "print(f\"  - Early Stopping: patience=7\")\n",
    "print(f\"  - Reduce LR on Plateau: factor=0.5, patience=3\")\n",
    "print(f\"  - Model Checkpoint: models/best_model.h5\")\n",
    "print(f\"  - CSV Logger: models/training_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(valid_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,  # Use validation set, not test set\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best model saved to: models/best_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3a1a8-174f-4464-800f-311b0bebbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history - Loss\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], color=\"teal\", label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color=\"orange\", label='Validation Loss')\n",
    "plt.title('Model Loss', fontsize=20)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00387c-5f78-4de4-97cd-f444beea6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history - Accuracy\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], color=\"teal\", label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color=\"orange\", label='Validation Accuracy')\n",
    "plt.title('Model Accuracy', fontsize=20)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c60ee-f27e-404e-9dd8-94c49c501242",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8f241-a0f0-4df5-a373-147a76fa8b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1416b0-6637-4513-9543-a2ef5417ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0878d-0cb0-4de7-9e51-8a6e9906b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "for batch in test_ds.as_numpy_iterator():\n",
    "    x, y = batch\n",
    "    yhat = model.predict(x, verbose=0)\n",
    "    pre.update_state(y, yhat)\n",
    "    re.update_state(y, yhat)\n",
    "    acc.update_state(y, yhat)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Precision: {pre.result().numpy():.4f}\")\n",
    "print(f\"  Recall: {re.result().numpy():.4f}\")\n",
    "print(f\"  Accuracy: {acc.result().numpy():.4f}\")\n",
    "\n",
    "# Get detailed classification report\n",
    "print(\"\\nDetailed Evaluation:\")\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for batch in test_ds.as_numpy_iterator():\n",
    "    x, y = batch\n",
    "    yhat = model.predict(x, verbose=0)\n",
    "    y_true.extend(y.flatten())\n",
    "    y_pred.extend((yhat > 0.5).astype(int).flatten())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['malignant', 'benign']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa3c11-9b43-4b8b-9887-e05e39eb3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics are already printed in the previous cell\n",
    "# Additional visualization can be added here if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb3129-a243-494c-ba80-3fad4809f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b1320-6726-443b-a3b6-cb0f77074f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing prediction on a single image\n",
    "\n",
    "img_path = '../data/raw/archive/BreaKHis_v1/histology_slides/breast/benign/SOB/fibroadenoma/SOB_B_F_14-14134E/200X/SOB_B_F-14-14134E-200-021.png'\n",
    "img = tf.io.read_file(img_path)\n",
    "img = tf.image.decode_image(img, channels=3)\n",
    "img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "img = img / 255.0\n",
    "img = tf.expand_dims(img, 0)\n",
    "\n",
    "\n",
    "prediction = model.predict(img)\n",
    "if prediction > 0.5:\n",
    "    print(\"Predicted class is: Benign\")\n",
    "else:\n",
    "    print(\"Predicted class is: Malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99820bdf-c2ac-45cb-b06f-a1952f098add",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660df09f-339b-449a-9df4-37461a2f49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6005f0",
   "metadata": {},
   "source": [
    "## Summary of Overfitting Prevention Techniques\n",
    "\n",
    "This notebook implements multiple techniques to prevent overfitting:\n",
    "\n",
    "1. **Data Augmentation**: \n",
    "   - Random flips (left-right, up-down)\n",
    "   - Random brightness, contrast, saturation, hue adjustments\n",
    "   - Random rotations\n",
    "   - Applied only to training data\n",
    "\n",
    "2. **Regularization**:\n",
    "   - L2 regularization on dense layers (0.001)\n",
    "   - Dropout layers (0.2, 0.5, 0.4, 0.3)\n",
    "   - Batch normalization after each dense layer\n",
    "   - Label smoothing (0.1) in loss function\n",
    "\n",
    "3. **Callbacks**:\n",
    "   - Early stopping (patience=7) to stop training when validation loss stops improving\n",
    "   - Reduce learning rate on plateau (factor=0.5, patience=3)\n",
    "   - Model checkpointing to save best model based on validation loss\n",
    "   - TensorBoard for monitoring training\n",
    "\n",
    "4. **Data Handling**:\n",
    "   - Proper train/validation/test split\n",
    "   - Class balancing through oversampling in training set\n",
    "   - Separate validation set (not using test set for validation)\n",
    "\n",
    "5. **Model Architecture**:\n",
    "   - Transfer learning with frozen EfficientNetV2-B0 base\n",
    "   - Progressive dropout (higher in early layers)\n",
    "   - Multiple dense layers with regularization\n",
    "   - Lower initial learning rate (0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285da49-aef7-4950-b6be-a0f571127d15",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0808d6d-03df-4f9f-a98d-7b16672131e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5740b62-abd5-4d5c-86ea-d5b6fedc38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model (best model is already saved by ModelCheckpoint callback)\n",
    "# Option 1: Save the current model state\n",
    "model.save(os.path.join('models', 'Breast_Cancer_Histopathology_Image_Classification_model.keras'))\n",
    "\n",
    "# Option 2: Load and save the best model from checkpoint\n",
    "# The best model based on validation loss is already saved as 'models/best_model.h5'\n",
    "# print(\"Model saved successfully!\")\n",
    "# print(\"Best model (based on validation loss): models/best_model.h5\")\n",
    "# print(\"Final model: models/Breast_Cancer_Histopathology_Image_Classification_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9da44-f958-4c3d-89bf-6fe24cf06f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model (saved by ModelCheckpoint callback)\n",
    "best_model = load_model('models/best_model.h5')\n",
    "print(\"Best model loaded successfully!\")\n",
    "\n",
    "# Or load the final model\n",
    "# final_model = load_model(os.path.join('models', 'Breast_Cancer_Histopathology_Image_Classification_model.keras'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a43b5e-1b59-4b7c-bc99-98b5635a9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on a single image\n",
    "# Replace 'path/to/image.png' with your actual image path\n",
    "# img_path = 'path/to/image.png'\n",
    "# img = tf.io.read_file(img_path)\n",
    "# img = tf.image.decode_image(img, channels=3)\n",
    "# img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "# img = img / 255.0\n",
    "# img = tf.expand_dims(img, 0)\n",
    "# \n",
    "# prediction = best_model.predict(img, verbose=0)\n",
    "# print(f\"Prediction probability: {prediction[0][0]:.4f}\")\n",
    "# if prediction[0][0] > 0.5:\n",
    "#     print(\"Predicted class is: Benign\")\n",
    "# else:\n",
    "#     print(\"Predicted class is: Malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ea1d7-01e8-480f-98f5-c93f7ebd14e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
